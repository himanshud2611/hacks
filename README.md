# RLM Paper Reproduction

**Reproducing "Recursive Language Models" (arXiv:2512.24601) from scratch**

## Overview

This repository contains a from-scratch implementation of the RLM (Recursive Language Model) inference strategy, along with comprehensive benchmarks and experiments.

## Key Findings

### âœ… What Works
- **1M character contexts** - RLM successfully handles contexts 5x larger than model window
- **Multi-hop reasoning** - Chains facts together correctly
- **Adversarial robustness** - Not fooled by decoy values
- **Aggregation tasks** - Accurate counting via code execution

### ðŸ”´ Failure Modes Discovered
- **Position 0.9 blind spot** - Struggles to find needles at very end of long contexts
- **JSON format confusion** - Gets confused by structured data with many numbers
- **No "not found" capability** - Won't admit when answer doesn't exist

## Results Summary

| Context Size | Direct Baseline | RLM |
|--------------|-----------------|-----|
| 100K | âœ“ | âœ“ |
| 300K | âœ— (truncated) | âœ“ |
| 500K | âœ— (truncated) | âœ“ |
| 1M | âœ— (truncated) | âœ“ |

## Structure

```
â”œâ”€â”€ BENCHMARK.md           # Experiment tracking
â”œâ”€â”€ FINDINGS.md            # Analysis report
â”œâ”€â”€ RESEARCH_HYPOTHESES.md # Edge case experiments
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ rlm.py         # Main RLM implementation
â”‚   â”‚   â”œâ”€â”€ repl.py        # Sandboxed REPL environment
â”‚   â”‚   â””â”€â”€ llm_client.py  # LLM client interfaces
â”‚   â”œâ”€â”€ benchmarks/
â”‚   â”‚   â”œâ”€â”€ niah.py        # Needle-in-haystack
â”‚   â”‚   â””â”€â”€ aggregation.py # Counting tasks
â”‚   â””â”€â”€ baselines/
â”‚       â”œâ”€â”€ direct.py      # Direct prompting
â”‚       â”œâ”€â”€ rag.py         # RAG baseline
â”‚       â””â”€â”€ chunked.py     # Chunk+aggregate
â”œâ”€â”€ experiments/           # Edge case experiments
â”œâ”€â”€ results/               # Raw JSON results
â””â”€â”€ test_*.py              # Test scripts
```

## Usage

```python
from src.core.rlm import RLM
from src.core.anannas_client import AnannasClient

client = AnannasClient(model="zai-org/glm-4.7")
rlm = RLM(root_client=client, verbose=True)

result = rlm.completion(
    context="<your long context here>",
    query="What is the secret information?"
)
print(result.answer)
```

## References

- Paper: https://arxiv.org/abs/2512.24601
- Official Repo: https://github.com/alexzhang13/rlm
- Blogpost: https://alexzhang13.github.io/blog/2025/rlm/

## License

MIT
