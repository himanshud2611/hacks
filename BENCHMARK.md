# RLM Paper Reproduction: Benchmark & Experiment Tracking

**Paper:** Recursive Language Models (arXiv:2512.24601)  
**Authors:** Alex L. Zhang, Tim Kraska, Omar Khattab (MIT CSAIL)  
**Reproduction Started:** 2025-01-16  
**Status:** âœ… Implementation Complete (API Testing Blocked)

---

## 1. Paper Summary

### Core Idea
Recursive Language Models (RLMs) are a **task-agnostic inference paradigm** that allows LLMs to handle near-infinite length contexts by enabling the model to:
1. **Offload context** into a REPL environment as a variable
2. **Programmatically examine** and decompose the context
3. **Recursively call itself** (sub-LLM queries) over chunks of the input
4. **Aggregate results** to produce a final answer

### Key Insight
Instead of trying to fit everything into a fixed context window, RLMs let the model:
- Store the full context as a Python variable (`context`)
- Write code to chunk, analyze, and query sub-LLMs
- Use `llm_query(prompt)` and `llm_query_batched(prompts)` functions
- Return final answer via `FINAL(answer)` or `FINAL_VAR(variable)`

### Architecture Components

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    RLM Completion                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. User provides: prompt (can be arbitrarily long)     â”‚
â”‚  2. Context offloaded to REPL as `context` variable     â”‚
â”‚  3. Root LLM generates code in ```repl``` blocks        â”‚
â”‚  4. Code executes in sandboxed environment              â”‚
â”‚  5. Sub-LLM calls via llm_query() / llm_query_batched() â”‚
â”‚  6. Iterate until FINAL() or max_iterations reached     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 2. Paper's Claimed Results

### Models Tested
- **GPT-5** (frontier closed model, 272K context window)
- **Qwen3-Coder-480B-A35B** (frontier open model)

### Benchmarks

| Benchmark | Description | Context Length | Task Type |
|-----------|-------------|----------------|-----------|
| **BrowseComp-Plus** | Multi-hop QA over 1K documents for DeepResearch | ~1K docs | Multi-hop reasoning |
| **S-NIAH** | Semantic Needle-in-a-Haystack (not just string matching) | 2^13 to 2^20 tokens | Retrieval + reasoning |
| **OOLONG** | Long reasoning benchmark requiring semantic transformation | 2^13 to 2^20 tokens | Aggregation |
| **OOLONG-Pairs** | Modified OOLONG with 20 queries requiring pair aggregation | Variable | Complex aggregation |

### Baselines Compared
1. **Direct prompting** - Fit everything in context window
2. **Chunk + Aggregate** - Split into chunks, process separately, aggregate
3. **RAG (BM25)** - Retrieve relevant chunks via BM25
4. **CodeAct** - Agent with code execution but no recursive sub-calls

### Key Claims

1. **RLMs outperform baselines** on all benchmarks, especially beyond context window limits
2. **Performance scales** with input length where other methods degrade
3. **Task-agnostic** - same system prompt works across all benchmarks
4. **Reasonable cost** - inference cost comparable to or lower than alternatives

### Results Table (from paper Table 1)

| Method | BrowseComp-Plus | S-NIAH (128K) | OOLONG | OOLONG-Pairs |
|--------|-----------------|---------------|--------|--------------|
| Direct | ? | ? | ? | ? |
| RAG | ? | ? | ? | ? |
| CodeAct | ? | ? | ? | ? |
| **RLM (GPT-5)** | **Best** | **Best** | **Best** | **Best** |
| **RLM (Qwen3)** | **Best** | **Best** | **Best** | **Best** |

*Note: Need to extract exact numbers from paper*

---

## 3. Reproduction Plan

### Phase 1: Understanding & Setup âœ… In Progress
- [x] Clone official repo
- [x] Read core implementation (`rlm/core/rlm.py`)
- [x] Understand system prompt and REPL mechanics
- [ ] Document exact benchmark datasets and metrics

### Phase 2: Implement from Scratch âœ… DONE
- [x] Implement minimal RLM core (no dependencies on official repo)
- [x] Implement REPL environment with `llm_query` function
- [x] Implement code parsing and execution
- [x] Implement FINAL() detection

**Implementation Files:**
- `src/core/rlm.py` - Main RLM class with completion loop
- `src/core/repl.py` - Sandboxed REPL environment
- `src/core/llm_client.py` - LLM client interfaces (Anthropic, OpenAI)

### Phase 3: Create Benchmarks âœ… DONE
- [x] Implement NIAH (Needle-in-a-Haystack) benchmark
- [x] Implement Aggregation benchmark (counting, comparison)
- [x] Create synthetic test cases for validation

**Benchmark Files:**
- `src/benchmarks/niah.py` - NIAH benchmark with varying context lengths, needle positions
- `src/benchmarks/aggregation.py` - Counting and comparison tasks across chunks

### Phase 4: Run Experiments ğŸŸ¡ IN PROGRESS
- [ ] Test with Claude Sonnet
- [ ] Compare RLM vs Direct prompting
- [ ] Compare RLM vs RAG baseline  
- [ ] Compare RLM vs Chunked baseline
- [ ] Measure: accuracy, cost, latency

**Baseline Files:**
- `src/baselines/direct.py` - Direct prompting (full context in prompt)
- `src/baselines/rag.py` - BM25 retrieval + generation
- `src/baselines/chunked.py` - Chunk + aggregate approach

### Phase 5: Analysis & Report
- [ ] Document what worked
- [ ] Document what didn't work
- [ ] Compare to paper's claims
- [ ] Identify limitations and insights

---

## 4. Implementation Notes

### Official Repo Structure (alexzhang13/rlm)

```
rlm/
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ rlm.py          # Main RLM class
â”‚   â”œâ”€â”€ lm_handler.py   # HTTP server for sub-LLM requests
â”‚   â”œâ”€â”€ types.py        # Type definitions
â”‚   â””â”€â”€ comms_utils.py  # Communication utilities
â”œâ”€â”€ environments/
â”‚   â”œâ”€â”€ local_repl.py   # Local Python exec environment
â”‚   â”œâ”€â”€ docker_repl.py  # Docker-based sandbox
â”‚   â”œâ”€â”€ modal_repl.py   # Modal cloud sandbox
â”‚   â””â”€â”€ prime_repl.py   # Prime Intellect sandbox
â”œâ”€â”€ clients/
â”‚   â”œâ”€â”€ openai.py       # OpenAI client
â”‚   â”œâ”€â”€ anthropic.py    # Anthropic client
â”‚   â””â”€â”€ ...             # Other providers
â””â”€â”€ utils/
    â”œâ”€â”€ prompts.py      # System prompts
    â””â”€â”€ parsing.py      # Code block parsing
```

### Key System Prompt Elements

1. **Context variable**: `context` contains the full input
2. **Sub-LLM functions**:
   - `llm_query(prompt)` - Single query, ~500K char limit
   - `llm_query_batched(prompts)` - Concurrent multiple queries
3. **Final answer**:
   - `FINAL(answer)` - Direct answer
   - `FINAL_VAR(variable)` - Return a variable's value
4. **Code execution**: Wrapped in ` ```repl ` blocks

### Execution Flow

```python
for i in range(max_iterations):
    response = lm.completion(prompt + history)
    code_blocks = find_code_blocks(response)
    
    for code in code_blocks:
        result = repl.execute(code)
        # Code can call llm_query() which makes HTTP request
        # to LMHandler server running in background
    
    if "FINAL" in response:
        return extract_final_answer(response)
    
    history.append(response + results)
```

---

## 5. Experiment Log

### Experiment 1: Mock Validation âœ… PASSED
**Date:** 2025-01-16  
**Objective:** Validate RLM implementation logic works correctly  
**Model:** Mock (no API calls)  
**Dataset:** Synthetic NIAH (8K chars)  
**Result:** SUCCESS

**Details:**
- Context: 7904 characters of random text with embedded "magic number" needle
- Query: "What is the secret magic number mentioned in the context?"
- Expected: 16863
- RLM Found: 16863 âœ“

**Iterations:**
1. Explored context structure (type, length, sample)
2. Used regex to search for pattern `secret magic number is (\d+)`
3. Found answer and returned `FINAL(16863)`

**Observations:**
- REPL environment correctly maintains state across code blocks
- Code block extraction (```repl ... ```) works properly
- FINAL() detection and extraction works
- Sub-LLM query function available to REPL code
- Variable persistence between code executions works

### Experiment 2: Component Tests âœ… PASSED
**Date:** 2025-01-16  
**Objective:** Unit test all components  
**Components Tested:**
- [x] NIAH sample generation
- [x] Aggregation sample generation
- [x] REPL code execution
- [x] REPL llm_query integration
- [x] Variable persistence in REPL
- [x] Benchmark evaluation functions
- [x] Code block regex extraction
- [x] FINAL() regex extraction
- [x] Mock LLM client

### Experiment 3: Real API Test âœ… PASSED
**Date:** 2025-01-16  
**API:** Anannas (OpenRouter-compatible)  
**Model:** `anthropic/claude-3.5-sonnet`

**Test 1: Small NIAH (2K chars)**
| Method | Correct | Time | Tokens |
|--------|---------|------|--------|
| Direct | âœ“ | 46s | 1,125 in / 16 out |
| RLM | âœ“ | 117s | multi-iteration |

**Test 2: Medium NIAH (5K chars)**  
| Metric | Value |
|--------|-------|
| Correct | âœ“ |
| Iterations | 4 |
| Time | 191s |
| Tokens | 7,932 in / 2,602 out |

**What Claude did in RLM mode:**
1. Explored context structure + called sub-LLM
2. Analyzed character/word frequencies, tried Caesar shifts
3. Found numbers via regex: `['93810']`
4. Verified context, returned `FINAL(93810)`

**Key Insight:** RLM was thorough (checked Base64, ciphers!) but overkill for this simple task. Direct prompting was faster and equally correct for small contexts.

### Experiment 4: 100K Context Test âœ…
**Date:** 2025-01-16  
**Model:** `zai-org/glm-4.7` (205K context window)

| Method | Correct | Time | Tokens In | Tokens Out |
|--------|---------|------|-----------|------------|
| Direct | âœ“ | 32.7s | 49,173 | 4 |
| RLM | âœ“ | 129.2s | 3,449 | 404 |

**Finding:** Both work at 100K. Direct faster, RLM more token-efficient (14x fewer input tokens).

### Experiment 5: 300K Context Test âœ… KEY RESULT
**Date:** 2025-01-16  
**Model:** `zai-org/glm-4.7`  
**Needle Position:** 0.9 (270K chars - beyond truncation!)

| Method | Correct | Time | Notes |
|--------|---------|------|-------|
| Direct | âœ˜ | 39.2s | Truncated to 200K, missed needle |
| RLM | âœ“ | 90.3s | Found needle at 270K in 3 iterations |

**This validates RLM's core value:** When context exceeds model window, Direct fails but RLM succeeds.

### Experiment 6: 500K Context Test âœ… STRESS TEST
**Date:** 2025-01-16  
**Model:** `zai-org/glm-4.7`  
**Needle Position:** 0.85 (425K chars)

| Method | Correct | Time | Iterations | Tokens |
|--------|---------|------|------------|--------|
| Direct | âœ˜ | 51.3s | N/A | ~100K in |
| RLM | âœ“ | 267s | 6 | 7,660 in / 738 out |

**RLM found the needle at 425K chars while using 13x fewer tokens than Direct (which failed).**

### Experiment 7: 1 MILLION CHAR TEST ğŸ†
**Date:** 2025-01-16  
**Model:** `zai-org/glm-4.7`  
**Context:** 999,904 chars (~250K tokens)  
**Needle Position:** 0.75 (750K chars)

| Method | Correct | Time | Iterations | Notes |
|--------|---------|------|------------|-------|
| Direct | âœ˜ | 45s | N/A | Truncated to 200K, missed needle |
| RLM | âœ“ | **14 min** | 6 | Found needle via batched sub-LLM queries |

**RLM Strategy:**
1. Explored context (type, length, sample)
2. Batched 5 sub-LLM queries on 200K chunks
3. Batched 5 more queries on different chunks
4. Found digits `['28235']` at index 749,921
5. Verified context around needle
6. Returned `FINAL(28235)` âœ“

**This is 5x the model's context window - and RLM handled it.**

### Experiment 8: Aggregation - Counting Task âœ…
**Date:** 2025-01-16  
**Task:** Count occurrences of 'banana' across 50 sections  
**Context:** 44,727 chars  
**Expected:** 18 mentions

| Method | Answer | Correct | Time |
|--------|--------|---------|------|
| Direct | 7 | âœ˜ | 57s |
| RLM | **18** | âœ” | 120s |

**Key finding:** Direct MISCOUNTED (said 7, actual 18). RLM used `context.count('banana')` for exact answer.

### Experiment 9: Aggregation - Comparison Task âœ…
**Date:** 2025-01-16  
**Task:** Find highest growth rate for Company D  
**Context:** 39,249 chars  
**Expected:** 8989

| Method | Answer | Correct | Time |
|--------|--------|---------|------|
| Direct | 8989 | âœ” | 6s |
| RLM | 8989 | âœ” | 93s |

**Key finding:** Both correct, but Direct faster when context fits in window. RLM used batched sub-queries to search 5 chunks in parallel.

---

## 6. Deep Research: Edge Cases & Failure Modes

### Experiment A: Position Sensitivity (300K context)

| Position | RLM Result |
|----------|------------|
| 0.1 (30K) | âœ“ |
| 0.3 (90K) | âœ“ |
| 0.5 (150K) | âœ“ |
| 0.7 (210K) | âœ“ |
| **0.9 (270K)** | **âœ— FAILED** |

**ğŸ”´ CRITICAL FINDING:** RLM has a **blind spot at position 0.9** in 300K contexts. The model's search strategy doesn't reliably reach the very end.

### Experiment B: Data Format Sensitivity

| Format | RLM Result |
|--------|------------|
| Random words | âœ“ |
| **JSON** | **âœ— FAILED** |

**ğŸ”´ FINDING:** Structured JSON with many numeric values confuses RLM - it returns wrong numbers.

### Experiment C: Multi-Needle âœ“ PASSED
RLM successfully finds individual facts scattered at positions 0.2, 0.5, 0.8.

### Experiment D: Adversarial Decoys âœ“ PASSED
RLM correctly identifies TRUE value despite 3 decoy values labeled as incorrect.

### Experiment E: No-Answer Scenario âœ— FAILED
RLM does NOT correctly say "not found" when needle doesn't exist. Instead describes context structure.

### Experiment F: Multi-Hop Reasoning âœ“ PASSED
All 1-hop and 2-hop queries passed. 2-hop doesn't require more iterations.

### Key Insights

**RLM Strengths:**
- Multi-hop reasoning
- Adversarial robustness
- Parallel fact retrieval

**RLM Weaknesses:**
- End-of-context blind spot (position 0.9)
- Structured data confusion (JSON)
- Won't admit "not found"

---

## 6. Key Observations

### What We've Learned So Far

1. **RLM is inference-only** - No training required, works with any LLM
2. **REPL is sandboxed** - Safe builtins only, blocks dangerous operations
3. **Sub-LLM calls are HTTP** - LMHandler runs a local server, REPL makes HTTP requests
4. **Supports multiple environments** - Local, Docker, Modal, Prime Intellect
5. **Max depth currently 1** - Only one level of recursion supported in current impl

### Implementation Insights (From Our Reproduction)

1. **System prompt is crucial** - The prompt teaches the model HOW to use the REPL
2. **Iterative loop** - Model generates code â†’ REPL executes â†’ results fed back â†’ repeat
3. **Code block detection** - Simple regex: ` ```repl\n(.*?)``` `
4. **Final answer detection** - `FINAL(answer)` or `FINAL_VAR(variable_name)`
5. **Context metadata** - Model told about context type/length upfront
6. **Truncated outputs** - REPL output truncated to prevent context overflow

### What Worked Well

âœ… **REPL approach is elegant** - Treating context as a variable the model can code against  
âœ… **llm_query as function** - Natural way to make recursive calls  
âœ… **Sandboxed execution** - Safe to let model write arbitrary code  
âœ… **Iterative refinement** - Model can observe results and adjust  
âœ… **Simple to implement** - Core logic is ~300 lines of Python  

### What Would Be Challenging

âš ï¸ **Token costs** - Each iteration uses root LLM + potential sub-LLM calls  
âš ï¸ **Latency** - Multiple round trips to LLM API  
âš ï¸ **Error handling** - Code execution failures need graceful recovery  
âš ï¸ **Model capability** - Requires strong code generation ability  
âš ï¸ **Prompt sensitivity** - System prompt significantly affects behavior  

### Open Questions

- [ ] What happens with deeply nested recursive calls?
- [ ] How does cost scale with context length?
- [ ] What's the failure mode when sub-LLM calls fail?
- [ ] How sensitive is performance to the system prompt?
- [ ] How does model size affect RLM effectiveness?

---

## 7. Files & Resources

| Resource | Location |
|----------|----------|
| Paper PDF | `rlm-reproduction/rlm_paper.pdf` |
| Official Repo | `rlm-reproduction/official-rlm/` |
| Our Implementation | `rlm-reproduction/src/` (TBD) |
| Benchmark Data | `rlm-reproduction/data/` (TBD) |
| Results | `rlm-reproduction/results/` (TBD) |

---

## 8. References

- Paper: https://arxiv.org/abs/2512.24601
- Official Repo: https://github.com/alexzhang13/rlm
- Blogpost: https://alexzhang13.github.io/blog/2025/rlm/
- RLM Minimal: https://github.com/alexzhang13/rlm-minimal

---

*Last Updated: 2025-01-16*
